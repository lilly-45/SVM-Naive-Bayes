{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & Naive Bayes\n",
        "\n",
        "\n",
        "1.  What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "   -> A Support Vector Machine (SVM) is a supervised machine learning algorithm that is mainly used for classification (and also for regression tasks, known as Support Vector Regression – SVR).\n",
        "\n",
        "It works by finding the optimal decision boundary (called a hyperplane) that separates data points of different classes with the maximum margin.\n",
        "\n",
        "\n",
        "2.  Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "    \n",
        "   -> Hard Margin SVM\n",
        "\n",
        "Definition:\n",
        "The classifier tries to find a hyperplane that perfectly separates the data, with no misclassifications allowed.\n",
        "\n",
        "Conditions:\n",
        "\n",
        "Works only when the dataset is linearly separable (classes can be perfectly divided by a straight line or hyperplane).\n",
        "\n",
        "Assumes no noise or outliers.\n",
        "\n",
        "   . Soft Margin SVM\n",
        "\n",
        "Definition:\n",
        "Allows some misclassifications by introducing slack variables\n",
        "\n",
        "Idea:\n",
        "\n",
        "Tries to maximize the margin while allowing some violations.\n",
        "\n",
        "This makes SVM more flexible and better for noisy, real-world data.\n",
        "\n",
        "\n",
        "3. What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "  -> Kernel Trick in SVM\n",
        "\n",
        "Problem:\n",
        "Many datasets are not linearly separable in their original space (you can’t just draw a straight line/hyperplane to separate the classes).\n",
        "\n",
        "Solution (Kernel Trick):\n",
        "Instead of manually mapping data into a higher-dimensional space (which can be computationally expensive), SVM uses a kernel function.\n",
        "\n",
        "A kernel implicitly computes the similarity (dot product) between data points in some higher-dimensional feature space.\n",
        "\n",
        "This allows SVM to find a linear separator in that higher-dimensional space without ever computing the transformation explicitly.\n",
        "\n",
        "xample: Radial Basis Function (RBF) Kernel\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∥\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝑥\n",
        "𝑗\n",
        "∥\n",
        "2\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")=exp(−γ∥x\n",
        "i\n",
        "\t​\n",
        "\n",
        "−x\n",
        "j\n",
        "\t​\n",
        "\n",
        "∥\n",
        "2\n",
        ")\n",
        "\n",
        "Intuition:\n",
        "\n",
        "Measures similarity between points based on distance.\n",
        "\n",
        "If two points are very close, the kernel value ≈ 1 (high similarity).\n",
        "\n",
        "If far apart, kernel value → 0 (low similarity).\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Useful for datasets where decision boundaries are non-linear and highly curved.\n",
        "\n",
        "Example: Handwritten digit recognition (like MNIST) — digits “8” vs. “3” overlap in raw pixel space, but using RBF kernel, the SVM can project data into a space where they become separable.\n",
        "\n",
        "\n",
        "4. What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "  -> A Naïve Bayes Classifier is a probabilistic supervised learning algorithm based on Bayes’ Theorem.\n",
        "It is commonly used for classification tasks (e.g., spam detection, sentiment analysis, medical diagnosis).\n",
        "\n",
        "Bayes’ Theorem:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(Y∣X)=\n",
        "P(X)\n",
        "P(X∣Y)⋅P(Y)\n",
        "\t​\n",
        "\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(Y∣X): Probability of class\n",
        "𝑌\n",
        "Y given features\n",
        "𝑋\n",
        "X (posterior).\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "P(X∣Y): Probability of features given the class (likelihood).\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        ")\n",
        "P(Y): Prior probability of the class.\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(X): Evidence (normalizing factor).\n",
        "\n",
        "The classifier predicts the class with the highest posterior probability.\n",
        "\n",
        "It assumes that all features are conditionally independent of each other given the class label.\n",
        "\n",
        "In real life, features are often correlated (e.g., in text classification, words often co-occur).\n",
        "\n",
        "But the algorithm ignores these dependencies, hence the term “naïve.”\n",
        "\n",
        "\n",
        "5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "  -> Gaussian Naïve Bayes\n",
        "\n",
        "Assumption:\n",
        "The features are continuous and follow a Gaussian (normal) distribution within each class.\n",
        "\n",
        "Likelihood formula:\n",
        "For feature\n",
        "𝑥\n",
        "x given class\n",
        "𝑦\n",
        "y:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "∣\n",
        "𝑦\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "𝜋\n",
        "𝜎\n",
        "𝑦\n",
        "2\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "(\n",
        "𝑥\n",
        "−\n",
        "𝜇\n",
        "𝑦\n",
        ")\n",
        "2\n",
        "2\n",
        "𝜎\n",
        "𝑦\n",
        "2\n",
        ")\n",
        "P(x∣y)=\n",
        "2πσ\n",
        "y\n",
        "2\n",
        "\t​\n",
        "\n",
        "\t​\n",
        "\n",
        "1\n",
        "\t​\n",
        "\n",
        "exp(−\n",
        "2σ\n",
        "y\n",
        "2\n",
        "\t​\n",
        "\n",
        "(x−μ\n",
        "y\n",
        "\t​\n",
        "\n",
        ")\n",
        "2\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "where\n",
        "𝜇\n",
        "𝑦\n",
        "μ\n",
        "y\n",
        "\t​\n",
        "\n",
        " and\n",
        "𝜎\n",
        "𝑦\n",
        "2\n",
        "σ\n",
        "y\n",
        "2\n",
        "\t​\n",
        "\n",
        " are the mean and variance of feature values for class\n",
        "𝑦\n",
        "y.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "When data has continuous values.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Medical data (blood pressure, cholesterol levels).\n",
        "\n",
        "Iris dataset (flower petal and sepal measurements).\n",
        "\n",
        "Multinomial Naïve Bayes\n",
        "\n",
        "Assumption:\n",
        "Features are discrete counts (e.g., number of times a word appears in a document).\n",
        "\n",
        "Likelihood formula:\n",
        "Probability of a document\n",
        "𝑑\n",
        "d given class\n",
        "𝑐\n",
        "c:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑑\n",
        "∣\n",
        "𝑐\n",
        ")\n",
        "=\n",
        "(\n",
        "∑\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "!\n",
        "∏\n",
        "𝑖\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "!\n",
        ")\n",
        "∏\n",
        "𝑖\n",
        "(\n",
        "𝜃\n",
        "𝑐\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "P(d∣c)=\n",
        "∏\n",
        "i\n",
        "\t​\n",
        "\n",
        "(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "!)\n",
        "(∑\n",
        "i\n",
        "\t​\n",
        "\n",
        "x\n",
        "i\n",
        "\t​\n",
        "\n",
        ")!\n",
        "\t​\n",
        "\n",
        "i\n",
        "∏\n",
        "\t​\n",
        "\n",
        "(θ\n",
        "ci\n",
        "x\n",
        "i\n",
        "\t​\n",
        "\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "where\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "\t​\n",
        "\n",
        " = count of feature\n",
        "𝑖\n",
        "i (e.g., word occurrences), and\n",
        "𝜃\n",
        "𝑐\n",
        "𝑖\n",
        "θ\n",
        "ci\n",
        "\t​\n",
        "\n",
        " = probability of feature\n",
        "𝑖\n",
        "i in class\n",
        "𝑐\n",
        "c.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Text classification (spam filtering, sentiment analysis).\n",
        "\n",
        "Document categorization (news classification).\n",
        "\n",
        "Bag-of-words or term frequency features.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "Assumption:\n",
        "Features are binary/boolean (present or absent, 0 or 1).\n",
        "\n",
        "Likelihood formula:\n",
        "For feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "\t​\n",
        "\n",
        " in document\n",
        "𝑑\n",
        "d:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝑦\n",
        ")\n",
        "=\n",
        "𝜃\n",
        "𝑦\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        "(\n",
        "1\n",
        "−\n",
        "𝜃\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "1\n",
        "−\n",
        "𝑥\n",
        "𝑖\n",
        "P(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "∣y)=θ\n",
        "yi\n",
        "x\n",
        "i\n",
        "\t​\n",
        "\n",
        "\t​\n",
        "\n",
        "(1−θ\n",
        "yi\n",
        "\t​\n",
        "\n",
        ")\n",
        "1−x\n",
        "i\n",
        "\t​\n",
        "\n",
        "\n",
        "where\n",
        "𝜃\n",
        "𝑦\n",
        "𝑖\n",
        "θ\n",
        "yi\n",
        "\t​\n",
        "\n",
        " = probability of feature\n",
        "𝑖\n",
        "i being present in class\n",
        "𝑦\n",
        "y.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Binary text classification (word present or not).\n",
        "\n",
        "Example: Spam classification based on presence/absence of words like “free,” “win,” “urgent.”\n",
        "\n",
        "Useful when only presence/absence matters, not frequency.\n",
        "\n",
        "6. Write a Python program to:\n",
        "\n",
        "● Load the Iris dataset\n",
        "\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "\n",
        "● Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "A1sD724QaANg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diyJARbrY8Tm",
        "outputId": "e00c4425-1117-49e5-d4bf-e1f1f8265a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data   # Features\n",
        "y = iris.target # Target labels\n",
        "\n",
        "# 2. Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train an SVM classifier with a linear kernel\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# 5. Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Print support vectors\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(svm_clf.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "ASwNTiJIllB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data   # Features\n",
        "y = data.target # Labels\n",
        "\n",
        "# 2. Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3z8uYAGlccR",
        "outputId": "81b09c12-e18b-4497-a948-9d4f503e1d0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy."
      ],
      "metadata": {
        "id": "Gd7kq4NSncov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']  # Using RBF kernel\n",
        "}\n",
        "\n",
        "# 4. Perform GridSearchCV\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 6. Make predictions and evaluate accuracy\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OydhEwNnaT2",
        "outputId": "4eaeda8b-f435-4ae5-9ad6-bf4bdb068064"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Set Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "aWSVSKLDoTHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. Load a subset of the 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['sci.space', 'rec.sport.baseball']  # two categories for binary task\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X = newsgroups.data   # text documents\n",
        "y = newsgroups.target # labels (0/1)\n",
        "\n",
        "# 2. Convert text to numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train a Multinomial Naïve Bayes classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict probabilities for ROC-AUC\n",
        "y_proba = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 6. Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQcHfR2qoRNj",
        "outputId": "4d30e9a4-a2db-4261-ea05-e9fbb43feba8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "\n",
        "   -> 1. Preprocessing the Data\n",
        "\n",
        "Since emails are text-heavy and may have missing values, preprocessing is critical.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Handling Missing Data:\n",
        "\n",
        "If the body/subject is missing → treat as empty string.\n",
        "\n",
        "If metadata (sender, timestamp) is missing → impute with placeholders or drop if not critical.\n",
        "\n",
        "Text Cleaning:\n",
        "\n",
        "Lowercasing, removing HTML tags, punctuation, and stopwords.\n",
        "\n",
        "Tokenization and stemming/lemmatization (optional for efficiency).\n",
        "\n",
        "Text Vectorization:\n",
        "\n",
        "Use TF-IDF Vectorizer → captures word importance relative to documents.\n",
        "\n",
        "Optionally include n-grams (e.g., bi-grams) to capture common spam phrases (“free money,” “click here”).\n",
        "\n",
        "Limit vocabulary size to manage dimensionality.\n",
        "\n",
        "2. Choosing the Model: SVM vs. Naïve Bayes\n",
        "\n",
        "Naïve Bayes (Multinomial/Bernoulli)\n",
        "\n",
        "Pros: Very fast, works well with text (word counts, presence/absence).\n",
        "\n",
        "Cons: Assumes feature independence (not always true), performance can plateau with complex patterns.\n",
        "\n",
        "SVM (with linear or RBF kernel)\n",
        "\n",
        "Pros: Handles high-dimensional sparse text features well, often achieves higher accuracy.\n",
        "\n",
        "Cons: Slower to train on very large datasets compared to Naïve Bayes.\n",
        "\n",
        "Choice:\n",
        "\n",
        "Start with Multinomial Naïve Bayes (baseline, fast).\n",
        "\n",
        "Then move to Linear SVM for production if resources allow, since it generally provides better precision-recall balance on spam tasks.\n",
        "\n",
        " 3. Addressing Class Imbalance\n",
        "\n",
        "In email datasets, legitimate emails (ham) usually far outnumber spam.\n",
        "\n",
        "Strategies:\n",
        "\n",
        "Class Weights: In SVM, set class_weight=\"balanced\" so minority class (spam) gets more importance.\n",
        "\n",
        "Resampling:\n",
        "\n",
        "Oversample spam (e.g., SMOTE).\n",
        "\n",
        "Undersample ham (carefully, to avoid losing valuable data).\n",
        "\n",
        "Threshold Tuning: Adjust decision threshold to increase recall for spam.\n",
        "\n",
        " 4. Evaluating Performance\n",
        "\n",
        "Accuracy alone is misleading for imbalanced data.\n",
        "\n",
        "Metrics to use:\n",
        "\n",
        "Precision: Of all predicted spam, how many are truly spam? (Avoids mislabeling legitimate emails).\n",
        "\n",
        "Recall (Sensitivity): Of all true spam, how many did we catch? (Avoids missing spam).\n",
        "\n",
        "F1-Score: Balance between precision and recall.\n",
        "\n",
        "ROC-AUC / PR-AUC: Especially useful with imbalance.\n",
        "\n",
        "Confusion Matrix: To see false positives (bad for user trust) vs. false negatives (spam leakage).\n",
        "\n",
        " 5. Business Impact\n",
        "\n",
        "Reduced Risk: Prevents phishing/malicious spam from reaching users → increased security.\n",
        "\n",
        "Improved Productivity: Less time wasted on junk emails → higher efficiency.\n",
        "\n",
        "Better Customer Trust: Users rely on the system to protect them from scams.\n",
        "\n",
        "Cost Savings: Automating spam detection reduces need for manual filtering."
      ],
      "metadata": {
        "id": "dDwavzDDozBd"
      }
    }
  ]
}